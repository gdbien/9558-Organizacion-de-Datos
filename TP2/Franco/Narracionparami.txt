Acá voy a narrar masomenos que fui haciendo, para luego pasarlo en limpio en el informe.
Lo primero fue agarrar el set de datos, y tratar de limpiarlo un poco, para que los modelos que lo utilicen, performen mejor. Modificaciones del estilo de dropear duplicados, o aquellas publicaciones que eran de prueba. Tambien otra limpieza (todavia no la realice) sería remover
los outliers (por ejemplo utilizando el zscore)
Una vez que el modelo estaba "limpio", probe realizar una primera prediccion utilizando XGBoost (sin rellenar los NaN's de ninguna manera), en 2 casos:
Sin variables categoricas (dropeando tipodepropiedad, ciudad y provincia): Utilice One-Hot-Encoding por separado a cada set.
Con variables categoricas
Los resultados de las predicciones fueron muy parecidas, y el tiempo que tardaban en cuanto iba aumentando la cantidad de estimadores, era sustancialmente cada vez mas grande (probando con n=10,100,1000), a mayor cantidad cantidad de estimadores, mejores resultados pero tardaba mas. Los parametros que mas cambiaban era el learning rate, el deapth y la cantidad de estimadores.

[0.1917405232363988, 0.33751562226118803, 0.9445817294972011, 0.7971123591050718, 0.7964682649915429] son los resultados del primer random forest que hice utilizando las 848 columnas
21min

[0.1962096407362667, 0.343756076967372, 0.9434938228937959, 0.7937285919377344, 0.7925440190348175] son los resultados si utilizo las 50 columnas mas importantes, pase de 21 min a 3 min de tiempo!!!!

[0.19261429816378212, 0.3389280081338887, 0.944506329607614, 0.796846217895984, 0.7959574240600801]
CPU times: user 5min 59s, sys: 761 ms, total: 6min
Wall time: 49.8 s
con 100 (tardo 2 min mas que con 50)

COn 60 features y tardo 3 min
[0.19465595668343758, 0.3418654942406615, 0.9440792623382561, 0.7951699226884271, 0.7943961550292457]

Con 10 features y tardo 1min
[0.2023189301481967, 0.3656250965001855, 0.9416662151928538, 0.8093854208657211, 0.8068768711390628]


Tomar un valor de columnas entre el 50 y 100!!
Los primeros XGBoost los probe con imputer, y sin imputer pero no hubo mucha diferencia.